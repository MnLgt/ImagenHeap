{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from PIL import Image\n",
    "\n",
    "from functools import lru_cache\n",
    "import torch\n",
    "import numpy as np\n",
    "from segment.dino_script import DinoResults\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from segment.utils import get_device\n",
    "\n",
    "DEVICE = get_device()\n",
    "\n",
    "CURDIR = os.getcwd()\n",
    "\n",
    "WEIGHTS_DIR = os.path.join(CURDIR, \"..\", \"weights\")\n",
    "\n",
    "# SAM\n",
    "SAM_CHECKPOINT = os.path.join(WEIGHTS_DIR, \"sam_vit_h_4b8939.pth\")\n",
    "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
    "\n",
    "\n",
    "# determine if tensor is empty\n",
    "def is_empty(tensor):\n",
    "    return tensor.size(0) == 0\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_sam_model():\n",
    "    # SAM PREDICTOR\n",
    "    sam = build_sam(checkpoint=SAM_CHECKPOINT)\n",
    "    return sam.to(device=DEVICE)\n",
    "\n",
    "\n",
    "def sam_transform_image(image, device=\"cpu\"):\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "\n",
    "    transform = ResizeLongestSide(1024)\n",
    "    image = transform.apply_image(image)\n",
    "    image = torch.as_tensor(image, device=device)\n",
    "    return image.permute(2, 0, 1).contiguous()\n",
    "\n",
    "\n",
    "def sam_prepare_boxes(boxes, image_size, device: str = DEVICE):\n",
    "    resize_transform = ResizeLongestSide(1024)\n",
    "    return resize_transform.apply_boxes_torch(boxes, image_size).to(device)\n",
    "\n",
    "\n",
    "def sam_prepare_row(image, boxes, image_size):\n",
    "    return dict(image=image, boxes=boxes, original_size=image_size)\n",
    "\n",
    "\n",
    "def sam_detect(sam_model, images, boxes, image_size=(1024, 1024)):\n",
    "\n",
    "    sam_boxes = [sam_prepare_boxes(box, image_size) for box in boxes]\n",
    "    batched_input = [\n",
    "        sam_prepare_row(image, box, image_size) for image, box in zip(images, sam_boxes)\n",
    "    ]\n",
    "    batched_output = sam_model(batched_input, multimask_output=False)\n",
    "    return batched_output\n",
    "\n",
    "\n",
    "def sam_process(results, sam_model, sam_images):\n",
    "    boxes, scores, phrases = results.boxes, results.scores, results.phrases\n",
    "    batch_size = sam_images.size(0)\n",
    "\n",
    "    # Create a mask for valid (non-empty) boxes\n",
    "    valid_mask = torch.tensor([not is_empty(box) for box in boxes], dtype=torch.bool)\n",
    "\n",
    "    # Initialize output with None dictionaries for all entries\n",
    "    output = [\n",
    "        {\"masks\": None, \"boxes\": None, \"scores\": None, \"phrases\": None}\n",
    "        for _ in range(batch_size)\n",
    "    ]\n",
    "\n",
    "    if valid_mask.any():\n",
    "        # Process only valid entries\n",
    "        valid_sam_images = sam_images[valid_mask]\n",
    "        valid_boxes = [box for box, is_valid in zip(boxes, valid_mask) if is_valid]\n",
    "\n",
    "        # Run SAM on valid images\n",
    "        sam_outputs = sam_detect(sam_model, valid_sam_images, valid_boxes)\n",
    "        valid_masks = [output.get(\"masks\") for output in sam_outputs]\n",
    "\n",
    "        # Filter scores and phrases manually\n",
    "        valid_scores = [\n",
    "            score for score, is_valid in zip(scores, valid_mask) if is_valid\n",
    "        ]\n",
    "        valid_phrases = [\n",
    "            phrase for phrase, is_valid in zip(phrases, valid_mask) if is_valid\n",
    "        ]\n",
    "\n",
    "        # Fill in the output for valid entries\n",
    "        valid_indices = valid_mask.nonzero().squeeze(1)\n",
    "        for i, (mask, box, score, phrase) in enumerate(\n",
    "            zip(valid_masks, valid_boxes, valid_scores, valid_phrases)\n",
    "        ):\n",
    "            output[valid_indices[i].item()] = {\n",
    "                \"masks\": mask,\n",
    "                \"boxes\": box,\n",
    "                \"scores\": score,\n",
    "                \"phrases\": phrase,\n",
    "            }\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_sam_results(images: List[Image.Image], dino_results: DinoResults, text_prompt: str, device: torch.device =DEVICE) -> List[dict]:\n",
    "    sam_model = get_sam_model()\n",
    "\n",
    "    # dino_images = torch.stack([transform_image_dino(image) for image in images])\n",
    "    sam_images = torch.stack([sam_transform_image(image) for image in images])\n",
    "\n",
    "    # dino_images = dino_images.to(DEVICE)\n",
    "    sam_images = sam_images.to(DEVICE)\n",
    "\n",
    "    unformatted_results = sam_process(dino_results, sam_model, sam_images)\n",
    "\n",
    "    return unformatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groundingdino",
   "language": "python",
   "name": "groundingdino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
