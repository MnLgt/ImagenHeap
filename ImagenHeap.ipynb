{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from segment.create_dataset import CreateSegmentationDataset\n",
    "from segment.utils import load_resize_image\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"datasets/fashion_people_detection/images/val\"\n",
    "\n",
    "ds = load_dataset(\"imagefolder\", data_dir=image_dir, split=\"train\")\n",
    "text_prompt = [\"face\", \"glasses\", \"clothes\"]\n",
    "ds = ds.shuffle().take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Union, Set\n",
    "from datasets import Dataset\n",
    "from segment.components.segment.SegmentSam import SegmentSam\n",
    "from segment.components.detect.DetectDino import DetectDino\n",
    "from segment.components.base import Component\n",
    "from segment.format_results import format_all_results\n",
    "from PIL import Image\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Dict, List, Union\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class DataManager:\n",
    "    def load_dataset(self, dataset: Union[str, Dataset], split=\"train\") -> Dataset:\n",
    "        # Placeholder for loading dataset\n",
    "        if isinstance(dataset, str):\n",
    "            print(f\"Loading dataset: {dataset}\")\n",
    "            return load_dataset(dataset, split=split)\n",
    "        else:\n",
    "            return dataset\n",
    "\n",
    "    def push_to_hub(self, repo_id, token, commit_message=\"md\", private=True):\n",
    "        create_repo(\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\",\n",
    "            exist_ok=True,\n",
    "            private=private,\n",
    "            token=token,\n",
    "        )\n",
    "\n",
    "        self.ds.push_to_hub(repo_id, commit_message=commit_message, token=token)\n",
    "\n",
    "        print(f\"Pushed Dataset to Hub: {repo_id}\")\n",
    "\n",
    "\n",
    "class TrainingManager:\n",
    "    def train_model(self, dataset: Dataset, model_config: Dict[str, Any]):\n",
    "        # Placeholder for model training\n",
    "        print(f\"Training model with config: {model_config}\")\n",
    "        print(f\"Using dataset: {dataset}\")\n",
    "\n",
    "\n",
    "class ComponentManager:\n",
    "    def __init__(self):\n",
    "        self.components: Dict[str, Component] = {}\n",
    "        self.pipeline: List[str] = []\n",
    "        self.loaded_components: Set[str] = set()\n",
    "\n",
    "    def register_component(self, component: Component):\n",
    "        self.components[component.name] = component\n",
    "\n",
    "    def get_component(self, name: str) -> Component:\n",
    "        return self.components.get(name)\n",
    "\n",
    "    def set_pipeline(self, pipeline: List[str]):\n",
    "        self.pipeline = pipeline\n",
    "\n",
    "    def validate_pipeline(self) -> bool:\n",
    "        for i in range(len(self.pipeline) - 1):\n",
    "            current_component = self.get_component(self.pipeline[i])\n",
    "            next_component = self.get_component(self.pipeline[i + 1])\n",
    "\n",
    "            if not all(\n",
    "                key in current_component.output_keys\n",
    "                for key in next_component.input_requirements\n",
    "            ):\n",
    "                print(\n",
    "                    f\"Invalid pipeline: {current_component.name} does not produce all required inputs for {next_component.name}\"\n",
    "                )\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def load_models(self):\n",
    "        \"\"\"\n",
    "        Loads the models for all components in the pipeline.\n",
    "        \"\"\"\n",
    "        for component_name in self.pipeline:\n",
    "            if component_name not in self.loaded_components:\n",
    "                component = self.get_component(component_name)\n",
    "                component.load_model()\n",
    "                self.loaded_components.add(component_name)\n",
    "\n",
    "    def unload_models(self):\n",
    "        \"\"\"\n",
    "        Unloads the models for all loaded components.\n",
    "        \"\"\"\n",
    "        for component_name in self.loaded_components:\n",
    "            component = self.get_component(component_name)\n",
    "            component.unload_model()\n",
    "        self.loaded_components.clear()\n",
    "\n",
    "\n",
    "    def process(self, initial_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        if not self.validate_pipeline():\n",
    "            raise ValueError(\"Invalid pipeline configuration\")\n",
    "\n",
    "        self.load_models()  # Load models before processing\n",
    "\n",
    "        try:\n",
    "            data = initial_data\n",
    "            for component_name in self.pipeline:\n",
    "                component = self.get_component(component_name)\n",
    "                data = component.process(data)\n",
    "            return data\n",
    "        finally:\n",
    "            self.unload_models()  # Ensure models are unloaded even if an exception occurs\n",
    "\n",
    "\n",
    "class ImagenHeap:\n",
    "    def __init__(self):\n",
    "        self.data_manager = DataManager()\n",
    "        self.component_manager = ComponentManager()\n",
    "        self.training_manager = TrainingManager()\n",
    "\n",
    "    def load_dataset(self, ds: Union[str, Dataset]) -> Dataset:\n",
    "        return self.data_manager.load_dataset(ds)\n",
    "\n",
    "    def process_dataset(\n",
    "        self, dataset: Dataset, text_prompt: Union[str, List[str]], **kwargs\n",
    "    ) -> Dataset:\n",
    "        # Prepare initial data for the pipeline\n",
    "        initial_data = {\"images\": dataset['image'], \"text_prompt\": text_prompt, **kwargs}\n",
    "\n",
    "        # Process the data through the pipeline\n",
    "        processed_data = self.component_manager.process(initial_data)\n",
    "\n",
    "        # Extract the final dataset from the processed data\n",
    "        return processed_data\n",
    "        \n",
    "\n",
    "    def train_model(self, dataset: Dataset, model_config: Dict[str, Any]):\n",
    "        self.training_manager.train_model(dataset, model_config)\n",
    "\n",
    "\n",
    "imagen_heap = ImagenHeap()\n",
    "\n",
    "# Register components\n",
    "imagen_heap.component_manager.register_component(DetectDino())\n",
    "imagen_heap.component_manager.register_component(SegmentSam())\n",
    "\n",
    "imagen_heap.component_manager.set_pipeline([\"detect\",\"segment\"])\n",
    "\n",
    "# Load dataset\n",
    "dataset = imagen_heap.load_dataset(ds)\n",
    "\n",
    "# Process dataset\n",
    "processed_dataset = imagen_heap.process_dataset(\n",
    "    dataset,\n",
    "    text_prompt=text_prompt,\n",
    "    max_image_side=1024,\n",
    "    box_threshold=0.3,\n",
    "    text_threshold=0.25,\n",
    "    iou_threshold=0.8,\n",
    "    return_tensors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = format_all_results(processed_dataset, polygons=False)\n",
    "results = [r.pop('polygons') for r in results]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groundingdino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
