{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from segment.sam_results import SAMResults\n",
    "from segment.utils import (get_coco_style_polygons, pad_to_fixed_size,\n",
    "                   resize_preserve_aspect_ratio)\n",
    "from segment.grounded_sam import (\n",
    "    run_grounded_sam_batch,\n",
    "    transform_image_dino,\n",
    "    transform_image_sam,\n",
    ")\n",
    "from segment.utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    return data\n",
    "\n",
    "def get_labels_dict(config_path):\n",
    "    data = load_yaml(config_path)\n",
    "    labels_dict = data.get(\"names\")\n",
    "    labels_dict = {v: k for k, v in labels_dict.items()}\n",
    "    return labels_dict\n",
    "\n",
    "config_path = \"configs/fashion_people_detection.yml\"\n",
    "data = load_yaml(config_path)\n",
    "labels_dict = get_labels_dict(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks_md(results):\n",
    "    results_list = []\n",
    "    for result in results.formatted_results:\n",
    "        mask = result.get(\"mask\")\n",
    "        coco_polygons = get_coco_style_polygons(mask)\n",
    "\n",
    "        # format the polygons\n",
    "        result.update({\"polygons\": coco_polygons})\n",
    "        result.pop(\"mask\")\n",
    "        results_list.append(result)\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [k for k, v in labels_dict.items()]\n",
    "text_prompt = \" . \".join(labels)\n",
    "text_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global cache_dir\n",
    "cache_dir = \"hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_pil(image_pil):\n",
    "    if image_pil.mode != \"RGB\":\n",
    "        image_pil = image_pil.convert(\"RGB\")\n",
    "    image_pil = resize_preserve_aspect_ratio(image_pil, 1024)\n",
    "    image_pil = pad_to_fixed_size(image_pil, (1024, 1024))\n",
    "    return image_pil\n",
    "\n",
    "\n",
    "class Segmentation(Dataset):\n",
    "    def __init__(self, dataset_id=None, image_col=\"image\", image_id_col=None):\n",
    "        self.ds = load_dataset(\n",
    "            dataset_id, split=\"train\", trust_remote_code=True, cache_dir=cache_dir, num_proc=os.cpu_count()\n",
    "        )\n",
    "        self.image_col = image_col\n",
    "        self.image_id_col = image_id_col\n",
    "        self.imgsz = 1024\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        # Get Image ID defaults to index\n",
    "        image_id = item.get(self.image_id_col, idx)\n",
    "\n",
    "        # Get PIL Image\n",
    "        image_pil = item[self.image_col]\n",
    "        # if image_pil.size[0] != self.imgsz or image_pil.size[1] != self.imgsz:\n",
    "        #     image_pil = resize_image_pil(image_pil)\n",
    "        image_pil = resize_image_pil(image_pil)\n",
    "\n",
    "\n",
    "        # Process dino image\n",
    "        dino_image = transform_image_dino(image_pil)\n",
    "\n",
    "        # Process sam image\n",
    "        sam_image = transform_image_sam(image_pil)\n",
    "\n",
    "        return {\"image_id\": image_id, \"dino_image\": dino_image, \"sam_image\": sam_image, \"image_pil\": image_pil}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the dataset ID and load it as a torch dataset\n",
    "dataset_id = \"jordandavis/fashion_num_people\"\n",
    "ds = Segmentation(dataset_id=dataset_id, image_col=\"image\", image_id_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.ds = ds.ds.filter(lambda x: x['num_people'] > 0)\n",
    "a = ds.ds['num_people']\n",
    "b = [i for i, x in enumerate(a) if x > 0]\n",
    "c = ds.ds.select(b)\n",
    "ds.ds = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ds = ds.ds.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def resize_image(examples):\n",
    "#     # Check if examples['image'] is a list of images or a single image\n",
    "#     if isinstance(examples['image'], list):\n",
    "#         # If it's a list, process each image\n",
    "#         examples['image'] = [resize_image_pil(image) for image in examples['image']]\n",
    "#     else:\n",
    "#         # If it's a single image, process the image directly\n",
    "#         examples['image'] = resize_image_pil(examples['image'])\n",
    "#     return examples\n",
    "\n",
    "# ds.ds = ds.ds.map(resize_image, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "def collate_fn(ex):\n",
    "    dino_images = torch.stack([e[\"dino_image\"] for e in ex])\n",
    "    sam_images = torch.stack([e[\"sam_image\"] for e in ex])\n",
    "    image_ids = [e[\"image_id\"] for e in ex]\n",
    "    pil_images = [e[\"image_pil\"] for e in ex]\n",
    "    return dict(image_ids=image_ids, dino_images=dino_images, sam_images=sam_images, pil_images=pil_images)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ds,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "masks_md = []\n",
    "with tqdm(total=len(dataloader)) as pbar:\n",
    "    for batch in dataloader:\n",
    "        image_ids = batch.get(\"image_ids\")\n",
    "        images = batch.get(\"pil_images\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dino_images = batch.get(\"dino_images\").to(device)\n",
    "            sam_images = batch.get(\"sam_images\").to(device)\n",
    "            raw_results = run_grounded_sam_batch(dino_images, sam_images, text_prompt)\n",
    "\n",
    "        for image_id, image, raw_result in zip(image_ids, images, raw_results):\n",
    "            if raw_result.get('masks') is None or ('person' not in raw_result.get(\"phrases\")):\n",
    "                mask_md = None\n",
    "            else:\n",
    "                result = SAMResults(image, labels_dict, **raw_result)\n",
    "                mask_md = get_masks_md(result)\n",
    "\n",
    "            masks_md.append(mask_md)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "# with open(\"results.json\", 'w') as f:\n",
    "#     f.write(json.dumps(masks_md, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_ds = ds.ds.take(len(masks_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Value\n",
    "from huggingface_hub import create_repo\n",
    "\n",
    "updated_ds = updated_ds.add_column(\"mask_metadata\", masks_md)\n",
    "# updated_ds = updated_ds.cast_column(\"width\", Value(\"int16\"))\n",
    "# updated_ds = updated_ds.cast_column(\"height\", Value(\"int16\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import convert_coco_polygons_to_mask, overlay_mask\n",
    "from PIL import Image \n",
    "\n",
    "def sanity_check(ds, row=102, mask_row=2):\n",
    "    image = ds[row]['image']\n",
    "    image = resize_image_pil(image)\n",
    "\n",
    "    polygons = ds[row]['mask_metadata'][mask_row]['polygons']\n",
    "    label = ds[row]['mask_metadata'][mask_row]['label']\n",
    "    \n",
    "    mask = convert_coco_polygons_to_mask(polygons, 1024, 1024)\n",
    "    mask_image = Image.fromarray(mask)\n",
    "    overlay = overlay_mask(image, mask_image, opacity=0.8)\n",
    "\n",
    "    print(label)\n",
    "    display(overlay)\n",
    "\n",
    "row = 3\n",
    "mask_row = 0\n",
    "sanity_check(updated_ds, row, mask_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with no mask_metadata\n",
    "a = updated_ds['mask_metadata']\n",
    "b = [i for i, x in enumerate(a) if x is not None]\n",
    "updated_ds = updated_ds.select(b)\n",
    "len(updated_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_repo_id = \"jordandavis/fashion_people_detections\"\n",
    "create_repo(\n",
    "    repo_id=new_repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    exist_ok=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_ds.push_to_hub(new_repo_id, commit_message=\"md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groundingdino",
   "language": "python",
   "name": "groundingdino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
