{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from scripts.sam_results import SAMResults\n",
    "from utils import (get_coco_style_polygons, pad_to_fixed_size,\n",
    "                   resize_preserve_aspect_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"
     ]
    }
   ],
   "source": [
    "from grounded_sam import (\n",
    "    run_grounded_sam_batch,\n",
    "    transform_image_dino,\n",
    "    transform_image_sam,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "def load_yaml(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    return data\n",
    "\n",
    "def get_labels_dict(config_path):\n",
    "    data = load_yaml(config_path)\n",
    "    labels_dict = data.get(\"names\")\n",
    "    labels_dict = {v: k for k, v in labels_dict.items()}\n",
    "    return labels_dict\n",
    "\n",
    "config_path = \"configs/fashion_people.yml\"\n",
    "data = load_yaml(config_path)\n",
    "labels_dict = get_labels_dict(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks_md(results):\n",
    "    results_list = []\n",
    "\n",
    "    for result in results.formatted_results:\n",
    "        mask = result.get(\"mask\")\n",
    "        coco_polygons = get_coco_style_polygons(mask)\n",
    "\n",
    "        # format the polygons\n",
    "        result.update({\"polygons\": coco_polygons})\n",
    "        result.pop(\"mask\")\n",
    "        results_list.append(result)\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hair . face . neck . arm . hand . back . leg . foot . outfit . person . phone'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [k for k, v in labels_dict.items()]\n",
    "text_prompt = \" . \".join(labels)\n",
    "text_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "global cache_dir\n",
    "cache_dir = \"../hf_datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_pil(image_pil):\n",
    "    if image_pil.mode != \"RGB\":\n",
    "        image_pil = image_pil.convert(\"RGB\")\n",
    "    image_pil = resize_preserve_aspect_ratio(image_pil, 1024)\n",
    "    image_pil = pad_to_fixed_size(image_pil, (1024, 1024))\n",
    "    return image_pil\n",
    "\n",
    "\n",
    "class Segmentation(Dataset):\n",
    "    def __init__(self, dataset_id=None, image_col=\"image\", image_id_col=None):\n",
    "        self.ds = load_dataset(\n",
    "            dataset_id, split=\"train\", trust_remote_code=True, cache_dir=cache_dir, num_proc=os.cpu_count()\n",
    "        )\n",
    "        self.image_col = image_col\n",
    "        self.image_id_col = image_id_col\n",
    "        self.imgsz = 1024\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        # Get Image ID defaults to index\n",
    "        image_id = item.get(self.image_id_col, idx)\n",
    "\n",
    "        # Get PIL Image\n",
    "        image_pil = item[self.image_col]\n",
    "        if image_pil.size[0] != self.imgsz or image_pil.size[1] != self.imgsz:\n",
    "            image_pil = resize_image_pil(image_pil)\n",
    "\n",
    "\n",
    "        # Process dino image\n",
    "        dino_image = transform_image_dino(image_pil)\n",
    "\n",
    "        # Process sam image\n",
    "        sam_image = transform_image_sam(image_pil)\n",
    "\n",
    "        return {\"image_id\": image_id, \"dino_image\": dino_image, \"sam_image\": sam_image, \"image_pil\": image_pil}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9313c6d97884aa3aeedfd46acdcd742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa118331182c45a7becf0ac2618f16f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/383 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb7aba2d2b1473e90f1b0e6a4f40ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/374 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enter the dataset ID and load it as a torch dataset\n",
    "dataset_id = \"jordandavis/fashion_num_people\"\n",
    "ds = Segmentation(dataset_id=dataset_id, image_col=\"image\", image_id_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.ds = ds.ds.filter(lambda x: x['num_people'] > 0)\n",
    "a = ds.ds['num_people']\n",
    "b = [i for i, x in enumerate(a) if x > 0]\n",
    "c = ds.ds.select(b)\n",
    "ds.ds = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ds = ds.ds.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ds = ds.ds.take(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0c9b6d0e7441c5a00fd78f6c3d6448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def resize_image(examples):\n",
    "    # Check if examples['image'] is a list of images or a single image\n",
    "    if isinstance(examples['image'], list):\n",
    "        # If it's a list, process each image\n",
    "        examples['image'] = [resize_image_pil(image) for image in examples['image']]\n",
    "    else:\n",
    "        # If it's a single image, process the image directly\n",
    "        examples['image'] = resize_image_pil(examples['image'])\n",
    "    return examples\n",
    "\n",
    "ds.ds = ds.ds.map(resize_image, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "def collate_fn(ex):\n",
    "    dino_images = torch.stack([e[\"dino_image\"] for e in ex])\n",
    "    sam_images = torch.stack([e[\"sam_image\"] for e in ex])\n",
    "    image_ids = [e[\"image_id\"] for e in ex]\n",
    "    pil_images = [e[\"image_pil\"] for e in ex]\n",
    "    return dict(image_ids=image_ids, dino_images=dino_images, sam_images=sam_images, pil_images=pil_images)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ds,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e885c40b7845ada43da1c3b5ac4ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "mask_metadata = {}\n",
    "masks_md = []\n",
    "with tqdm(total=len(dataloader)) as pbar:\n",
    "    for batch in dataloader:\n",
    "        image_ids = batch.get(\"image_ids\")\n",
    "        images = batch.get(\"pil_images\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dino_images = batch.get(\"dino_images\").to(device)\n",
    "            sam_images = batch.get(\"sam_images\").to(device)\n",
    "            raw_results = run_grounded_sam_batch(dino_images, sam_images, text_prompt)\n",
    "\n",
    "        for image_id, image, raw_result in zip(image_ids, images, raw_results):\n",
    "            if raw_result.get('masks') is None or ('person' not in raw_result.get(\"phrases\")):\n",
    "                # mask_md_row = dict(zip(str(image_id), [None]))\n",
    "                mask_md = None\n",
    "            else:\n",
    "                result = SAMResults(image, labels_dict, **raw_result)\n",
    "                mask_md = get_masks_md(result)\n",
    "\n",
    "            # mask_metadata.update(mask_md_row)\n",
    "            masks_md.append(mask_md)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect() \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1a804508374e2fa394891586f192ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a36a902931447d5a6044b060b9915fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Value\n",
    "from huggingface_hub import create_repo\n",
    "updated_ds = ds.ds\n",
    "updated_ds = updated_ds.add_column(\"mask_metadata\", masks_md)\n",
    "updated_ds = updated_ds.cast_column(\"width\", Value(\"int16\"))\n",
    "updated_ds = updated_ds.cast_column(\"height\", Value(\"int16\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456873"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out rows with no mask_metadata\n",
    "a = updated_ds['mask_metadata']\n",
    "b = [i for i, x in enumerate(a) if x is not None]\n",
    "updated_ds= updated_ds.select(b)\n",
    "len(updated_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_repo_id = \"jordandavis/fashion_people_detections\"\n",
    "# create_repo(\n",
    "#     repo_id=new_repo_id,\n",
    "#     repo_type=\"dataset\",\n",
    "#     exist_ok=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d81abfe19fd45468061de60def59226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88662565f0394b0fb71cd180603a89a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/467 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14331fe9e150457dbabbeb06e239fef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc3203db7c241daa8ca2e10f1cb8d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da256314888149f28af20dc8834cfefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac4463d3b2d484e8d94517f9fda07ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831c91684492451882726870b1a4f918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3756d6d11e4cc6806a3bbf07cb3b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5951ce6bbe14642bde30623f5544b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jordandavis/fashion_people_detections/commit/f6e7005ab0520b514c7304f165b7b6ec8290c072', commit_message='md', commit_description='', oid='f6e7005ab0520b514c7304f165b7b6ec8290c072', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_ds.push_to_hub(new_repo_id, commit_message=\"md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groundingdino",
   "language": "python",
   "name": "groundingdino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
