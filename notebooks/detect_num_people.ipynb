{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from datasets import Dataset as HF_Dataset\n",
    "from datasets import Features, Value, concatenate_datasets, load_dataset\n",
    "from utils import resize_preserve_aspect_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_people(result):\n",
    "    if bool(result):\n",
    "        labels_dict = result.names\n",
    "        det_labels = [int(i.item()) for i in result.boxes.cls]\n",
    "        det_labels = [labels_dict[i] for i in det_labels]\n",
    "        num_people = len(det_labels)\n",
    "        return num_people\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_fixed_size(img, size=(640, 640)):\n",
    "    width, height = img.size\n",
    "    # Calculate padding\n",
    "    left = (size[0] - width) // 2\n",
    "    top = (size[1] - height) // 2\n",
    "    right = size[0] - width - left\n",
    "    bottom = size[1] - height - top\n",
    "\n",
    "    # Apply padding\n",
    "    img_padded = F.pad(img, padding=(left, top, right, bottom))\n",
    "    return img_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"jordandavis/fashion\"\n",
    "ds = load_dataset(dataset_id, split=\"train\", trust_remote_code=True, num_proc=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinterestDataset(Dataset):\n",
    "    def __init__(self, dataset_id=None, image_col=\"image\", image_id_col=None):\n",
    "        self.ds = load_dataset(dataset_id, split=\"train\", trust_remote_code=True, num_proc=os.cpu_count())\n",
    "        self.image_col = image_col\n",
    "        self.image_id_col = image_id_col\n",
    "        self.imgsz = 640\n",
    "        self.half = False\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Lambda(lambda img: pad_to_fixed_size(img, (640, 640))),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "        image_pil = item[self.image_col]\n",
    "\n",
    "        if image_pil.mode != \"RGB\":\n",
    "            image_pil = image_pil.convert(\"RGB\")\n",
    "\n",
    "        image = resize_preserve_aspect_ratio(image_pil, self.imgsz)\n",
    "        image = self.transform(image).unsqueeze(0)\n",
    "\n",
    "        image_id = item.get(self.image_id_col, idx)\n",
    "\n",
    "        return {\"image_id\": image_id, \"image\": image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLO Model\n",
    "path = \"weights/yolov8n-seg.pt\"\n",
    "model = YOLO(path, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the dataset ID and load it as a torch dataset\n",
    "dataset_id = \"jordandavis/fashion\"\n",
    "ds = PinterestDataset(dataset_id=dataset_id, image_col=\"image\", image_id_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "\n",
    "\n",
    "def collate_fn(ex):\n",
    "    images = torch.cat([e[\"image\"] for e in ex], dim=0)\n",
    "    image_ids = [e[\"image_id\"] for e in ex]\n",
    "    return dict(images=images, image_ids=image_ids)\n",
    "\n",
    "\n",
    "workers = os.cpu_count()\n",
    "batch_size = 256\n",
    "dataloader = DataLoader(\n",
    "    ds,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=16,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_people_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_people = 0\n",
    "with tqdm(total=len(dataloader)) as pbar:\n",
    "    for batch in dataloader:\n",
    "        image_ids = batch.get(\"image_ids\")\n",
    "        with torch.no_grad():\n",
    "            images = batch.get(\"images\").to(\"cuda\")\n",
    "            results = model(images, classes=0, verbose=False)\n",
    "\n",
    "        num_people = [get_num_people(result) for result in results]\n",
    "        max_people_batch = max(num_people)\n",
    "        if max_people_batch > max_people:\n",
    "            max_people = max_people_batch\n",
    "            print(f\"Max people detected: {max_people}\", end=\"\\r\")\n",
    "\n",
    "        result = dict(zip(image_ids, num_people))\n",
    "        num_people_results.update(result)\n",
    "        pbar.update(1)\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"num_people_results.json\", \"w\") as f:\n",
    "    f.write(json.dumps(num_people_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results into a pandas dataframe\n",
    "df = pd.DataFrame(num_people_results.items(), columns=[\"image_id_processed\", \"num_people\"])\n",
    "\n",
    "# Convert that to a HF Dataset\n",
    "ds_people = HF_Dataset.from_pandas(df[['num_people']])\n",
    "\n",
    "# Concatenate the two datasets\n",
    "new_ds = concatenate_datasets([ds.ds, ds_people], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = new_ds.cast_column(\"width\", Value('int64'))\n",
    "new_ds = new_ds.cast_column(\"height\", Value('int64'))\n",
    "new_ds = new_ds.cast_column(\"id\", Value('int64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, upload_folder\n",
    "\n",
    "repo_id = \"jordandavis/fashion_num_people\"\n",
    "\n",
    "create_repo(\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    exist_ok=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_description = \"Add the num_people column\"\n",
    "new_ds.push_to_hub(repo_id, commit_description=commit_description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
