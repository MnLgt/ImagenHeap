{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('..')\n",
    "import random\n",
    "from PIL import Image\n",
    "import requests\n",
    "from utils import *\n",
    "import yaml\n",
    "from diffusers.utils import load_image, make_image_grid \n",
    "import numpy as np \n",
    "from ultralytics import YOLO\n",
    "from scripts.sam_results import SAMResults\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "yaml_file = \"../configs/fashion_people_detection.yml\"\n",
    "with open(yaml_file, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "labels_dict = config.get(\"names\")\n",
    "labels_dict_reversed = {v: k for k, v in labels_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"jordandavis/fashion_people_detections\"\n",
    "\n",
    "ds = load_dataset(dataset_id, split='train', trust_remote_code=True, cache_dir='../hf_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/workspace/SEGMENT/human_parsing/train2/weights/best.pt\"\n",
    "model = YOLO(model_path, task=\"segment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_num = random.randint(0, len(ds))\n",
    "print(f\"Row Num: {row_num}\")\n",
    "row = ds[row_num]\n",
    "image = row[\"image\"]\n",
    "image = resize_image_pil(image)\n",
    "image.resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://pbs.twimg.com/media/BorTTDoIIAAqFx5.jpg\"\n",
    "image = load_image(url)\n",
    "# image = resize_image_pil(image, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgsz = max(image.size)\n",
    "\n",
    "results = model(image, retina_masks=True, imgsz=imgsz)\n",
    "result = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unload(result):\n",
    "    masks = result.masks.data\n",
    "    boxes = result.boxes.xyxy\n",
    "    scores = result.boxes.conf\n",
    "\n",
    "    labels = result.boxes.cls\n",
    "    labels = [int(label.item()) for label in labels]\n",
    "    phrases = [labels_dict[label] for label in labels]\n",
    "    return masks, boxes, scores, phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks, boxes, scores, phrases = unload(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = SAMResults(image, labels_dict_reversed, masks=masks, boxes=boxes, scores=scores, phrases=phrases, person_masks_only=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.display_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = r.get_mask(\"phone\")[0]\n",
    "box = row.get('box')\n",
    "mask = row.get(\"mask\")\n",
    "overlay = overlay_mask(image, mask, opacity=0.9)\n",
    "overlay.crop(box).resize((512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
