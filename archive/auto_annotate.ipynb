{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.data.annotator import auto_annotate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/7 /workspace/SEGMENT/datasets/fashion_people_detection/small_dataset/images/0a0ea6ff29d185647e1bf71f1f374885.jpg: 640x640 1 hair, 1 face, 2 necks, 3 hands, 1 outfit, 20.7ms\n",
      "image 2/7 /workspace/SEGMENT/datasets/fashion_people_detection/small_dataset/images/0a1276e0bcf2e2c7d2c64475360a969a.jpg: 640x640 1 hair, 1 face, 1 foot, 1 outfit, 9.7ms\n",
      "image 3/7 /workspace/SEGMENT/datasets/fashion_people_detection/small_dataset/images/0a20feb27d1e1550d20855d150143239.jpg: 640x640 1 hair, 1 face, 1 neck, 2 arms, 2 hands, 1 leg, 3 foots, 1 outfit, 18.1ms\n",
      "image 4/7 /workspace/SEGMENT/datasets/fashion_people_detection/small_dataset/images/0a578004b348fa74a05c3e82b391644f.jpg: 640x640 1 hair, 1 face, 1 neck, 1 hand, 2 legs, 2 foots, 1 outfit, 21.9ms\n",
      "image 5/7 /workspace/SEGMENT/datasets/fashion_people_detection/small_dataset/images/0a5ace438efefacfe2adee37b263628b.jpg: 640x640 1 hair, 1 face, 1 neck, 2 arms, 1 hand, 2 foots, 1 outfit, 18.7ms\n",
      "image 6/7 /workspace/SEGMENT/datasets/fashion_people_detection/small_dataset/images/0a5afd35080ec0f7ce2432f5de7dfb8e.jpg: 640x640 1 hair, 1 face, 2 necks, 1 hand, 1 leg, 1 outfit, 20.7ms\n",
      "image 7/7 /workspace/SEGMENT/datasets/fashion_people_detection/small_dataset/images/0a9ab2a2c59833d37e77b4e578228ae1.jpg: 640x640 1 hair, 1 face, 1 neck, 1 arm, 1 hand, 2 legs, 1 outfit, 17.6ms\n",
      "Speed: 3.0ms preprocess, 18.2ms inference, 11.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/workspace/SEGMENT/datasets/fashion_people_detection/small_dataset\"\n",
    "\n",
    "weights_dir = \"/workspace/SEGMENT/weights\"\n",
    "sam_model = os.path.join(weights_dir, 'mobile_sam.pt')\n",
    "det_model = os.path.join(weights_dir, 'yolov8n.pt')\n",
    "det_model = \"/workspace/SEGMENT/human_parsing_new/train/weights/last.pt\"\n",
    "\n",
    "images_dir = os.path.join(data_dir, \"images\")\n",
    "output_dir = os.path.join(data_dir, 'annotations')\n",
    "\n",
    "auto_annotate(data=images_dir, det_model=det_model, sam_model=sam_model, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
