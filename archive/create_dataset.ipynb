{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from segment.sam_results import SAMResults\n",
    "from segment.utils import (get_coco_style_polygons, pad_to_fixed_size,\n",
    "                   resize_preserve_aspect_ratio)\n",
    "from segment.grounded_sam import (\n",
    "    run_grounded_sam_batch,\n",
    "    transform_image_dino,\n",
    "    transform_image_sam,\n",
    ")\n",
    "from segment.utils import get_device\n",
    "\n",
    "import json\n",
    "from datasets import Value\n",
    "from huggingface_hub import create_repo\n",
    "from segment.utils import convert_coco_polygons_to_mask, overlay_mask\n",
    "from PIL import Image\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Purpose: \n",
    "- Get all masks related to text prompts from a dataset and push the new dataset to ðŸ¤—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    return data\n",
    "\n",
    "def get_labels_dict(config_path):\n",
    "    data = load_yaml(config_path)\n",
    "    labels_dict = data.get(\"names\")\n",
    "    labels_dict = {v: k for k, v in labels_dict.items()}\n",
    "    return labels_dict\n",
    "\n",
    "\n",
    "def get_masks_md(results):\n",
    "    results_list = []\n",
    "    for result in results.formatted_results:\n",
    "        mask = result.get(\"mask\")\n",
    "        coco_polygons = get_coco_style_polygons(mask)\n",
    "\n",
    "        # format the polygons\n",
    "        result.update({\"polygons\": coco_polygons})\n",
    "        result.pop(\"mask\")\n",
    "        results_list.append(result)\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hair . face . neck . arm . hand . back . leg . foot . outfit . phone . hat . glasses . bag'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the cache dir\n",
    "cache_dir = \"hf_cache\"\n",
    "\n",
    "# Torch Settings\n",
    "batch_size = 8\n",
    "num_workers = os.cpu_count()\n",
    "device = get_device()\n",
    "\n",
    "# Get the labels from the yolo config file\n",
    "config_path = \"configs/fashion_people_detection_no_person.yml\"\n",
    "\n",
    "data = load_yaml(config_path)\n",
    "\n",
    "labels_dict = get_labels_dict(config_path)\n",
    "labels = [k for k, v in labels_dict.items()]\n",
    "text_prompt = \" . \".join(labels)\n",
    "text_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the dataset ID and load it as a torch dataset\n",
    "dataset_id = \"MnLgt/fashion_people_detections\"\n",
    "split='train[:100]'\n",
    "\n",
    "# Enter the repo name to push the new dataset to\n",
    "new_repo_id = \"jordandavis/fashion_people_detections\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image_pil(image_pil):\n",
    "    if image_pil.mode != \"RGB\":\n",
    "        image_pil = image_pil.convert(\"RGB\")\n",
    "    image_pil = resize_preserve_aspect_ratio(image_pil, 1024)\n",
    "    image_pil = pad_to_fixed_size(image_pil, (1024, 1024))\n",
    "    return image_pil\n",
    "\n",
    "\n",
    "class Segmentation(Dataset):\n",
    "    def __init__(self, dataset_id=None, image_col=\"image\", image_id_col=None, split='train'):\n",
    "        self.ds = load_dataset(\n",
    "            dataset_id, split=split, trust_remote_code=True, cache_dir=cache_dir, num_proc=os.cpu_count()\n",
    "        )\n",
    "        self.image_col = image_col\n",
    "        self.image_id_col = image_id_col\n",
    "        self.imgsz = 1024\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ds[idx]\n",
    "\n",
    "        # Get Image ID defaults to index\n",
    "        image_id = item.get(self.image_id_col, idx)\n",
    "\n",
    "        # Get PIL Image\n",
    "        image_pil = item[self.image_col]\n",
    "        # if image_pil.size[0] != self.imgsz or image_pil.size[1] != self.imgsz:\n",
    "        #     image_pil = resize_image_pil(image_pil)\n",
    "        image_pil = resize_image_pil(image_pil)\n",
    "\n",
    "\n",
    "        # Process dino image\n",
    "        dino_image = transform_image_dino(image_pil)\n",
    "\n",
    "        # Process sam image\n",
    "        sam_image = transform_image_sam(image_pil)\n",
    "\n",
    "        return {\"image_id\": image_id, \"dino_image\": dino_image, \"sam_image\": sam_image, \"image_pil\": image_pil}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9556c89244460a87440663e9f1e2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa52776834a341fe9f83503090c74661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = Segmentation(dataset_id=dataset_id, image_col=\"image\", image_id_col=None, split=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.ds = ds.ds.filter(lambda x: x['num_people'] > 0)\n",
    "# a = ds.ds['num_people']\n",
    "# b = [i for i, x in enumerate(a) if x > 0]\n",
    "# c = ds.ds.select(b)\n",
    "# ds.ds = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "def collate_fn(ex):\n",
    "    dino_images = torch.stack([e[\"dino_image\"] for e in ex])\n",
    "    sam_images = torch.stack([e[\"sam_image\"] for e in ex])\n",
    "    image_ids = [e[\"image_id\"] for e in ex]\n",
    "    pil_images = [e[\"image_pil\"] for e in ex]\n",
    "    return dict(image_ids=image_ids, dino_images=dino_images, sam_images=sam_images, pil_images=pil_images)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ds,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6814d400bd4c35bdc84d237df8b9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "masks_md = []\n",
    "with tqdm(total=len(dataloader)) as pbar:\n",
    "    for batch in dataloader:\n",
    "        image_ids = batch.get(\"image_ids\")\n",
    "        images = batch.get(\"pil_images\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dino_images = batch.get(\"dino_images\").to(device)\n",
    "            sam_images = batch.get(\"sam_images\").to(device)\n",
    "            raw_results = run_grounded_sam_batch(dino_images, sam_images, text_prompt)\n",
    "\n",
    "        for image_id, image, raw_result in zip(image_ids, images, raw_results):\n",
    "            if raw_result.get('masks') is None or ('person' not in raw_result.get(\"phrases\")):\n",
    "                mask_md = None\n",
    "            else:\n",
    "                result = SAMResults(image, labels_dict, **raw_result)\n",
    "                mask_md = get_masks_md(result)\n",
    "\n",
    "            masks_md.append(mask_md)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_ds = ds.ds.take(len(masks_md))\n",
    "updated_ds = updated_ds.add_column(\"mask_metadata2\", masks_md)\n",
    "# updated_ds = updated_ds.cast_column(\"width\", Value(\"int16\"))\n",
    "# updated_ds = updated_ds.cast_column(\"height\", Value(\"int16\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/SPAICE/SEGMENT/create_dataset.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B150.136.213.19/home/ubuntu/SPAICE/SEGMENT/create_dataset.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m metadata_col \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmask_metadata2\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B150.136.213.19/home/ubuntu/SPAICE/SEGMENT/create_dataset.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m row \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(updated_ds))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B150.136.213.19/home/ubuntu/SPAICE/SEGMENT/create_dataset.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m mask_row \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39;49m(updated_ds[row][metadata_col]) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B150.136.213.19/home/ubuntu/SPAICE/SEGMENT/create_dataset.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m sanity_check(updated_ds, row, mask_row, metadata_col\u001b[39m=\u001b[39mmetadata_col)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "def sanity_check(ds, row=102, mask_row=2,image_col=\"image\", metadata_col='mask_metadata'):\n",
    "    image = ds[row][image_col]\n",
    "    image = resize_image_pil(image)\n",
    "\n",
    "    polygons = ds[row][metadata_col][mask_row]['polygons']\n",
    "    label = ds[row][metadata_col][mask_row]['label']\n",
    "    score = ds[row][metadata_col][mask_row]['score']\n",
    "    \n",
    "    mask = convert_coco_polygons_to_mask(polygons, 1024, 1024)\n",
    "    mask_image = Image.fromarray(mask)\n",
    "    overlay = overlay_mask(image, mask_image, opacity=0.8)\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    display(overlay.resize((512, 512)))\n",
    "\n",
    "metadata_col = 'mask_metadata2'\n",
    "row = random.randint(0, len(updated_ds))\n",
    "mask_row = random.randint(0, len(updated_ds[row][metadata_col]) - 1)\n",
    "sanity_check(updated_ds, row, mask_row, metadata_col=metadata_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push To Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter out rows with no mask_metadata\n",
    "# a = updated_ds['mask_metadata']\n",
    "# b = [i for i, x in enumerate(a) if x is not None]\n",
    "# updated_ds = updated_ds.select(b)\n",
    "# ds_len = len(updated_ds)\n",
    "# print(f\"Dataset Length: {ds_len}\")\n",
    "\n",
    "# create_repo(\n",
    "#     repo_id=new_repo_id,\n",
    "#     repo_type=\"dataset\",\n",
    "#     exist_ok=True,\n",
    "# )\n",
    "\n",
    "# updated_ds.push_to_hub(new_repo_id, commit_message=\"md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "groundingdino",
   "language": "python",
   "name": "groundingdino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
